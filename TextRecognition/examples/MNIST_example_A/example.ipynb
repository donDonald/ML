{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2626d68d-32a9-4d8e-97a6-ba88d2a69ca8",
   "metadata": {},
   "source": [
    "# 1. Intro\n",
    "This example is powered by [https://www.kaggle.com/code/amyjang/tensorflow-mnist-cnn-tutorial](https://www.kaggle.com/code/amyjang/tensorflow-mnist-cnn-tutorial)\\\n",
    "This example is an introduction to Convolutional Neural Networks using TensorFlow 2.x Keras API.\\\n",
    "The dataset for training and validation is the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset, a dataset of handwritten digits 0-9, and here I will use a Sequential CNN to guess which digit was drawn.\\\n",
    "This ***model*** setup reaches 99.3% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9561c-88e2-4d46-8f0a-bf0c7e70d241",
   "metadata": {},
   "source": [
    "Import all mandatory packages ***?and change the accelerator from None to GPU?***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb2ad7-7554-4b3c-9ffd-b4cd283cdf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpltl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "print(f\"tensorflow:{tf.__version__}\")\n",
    "print(f\"seaborn:{sns.__version__}\")\n",
    "print(f\"numpy:{np.__version__}\")\n",
    "print(f\"pandas:{pd.__version__}\")\n",
    "print(f\"matplotlib:{mpltl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22287baf-d901-4c68-80b0-2eebc6f82dac",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing\n",
    "Before building any ML model, it is important to preprocess the data.\\\n",
    "In fact, data preprocessing will generally take up the most time in any ML pipeline.\\\n",
    "The following module goes over the steps to preprocess the MNIST dataset for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf36d44-31c9-43f2-8572-e3589b3f7c67",
   "metadata": {},
   "source": [
    "## 2.1 Load Data\n",
    "The first step is to load the data and divide it into a training and testing dataset.\\\n",
    "The MNIST dataset can be downloaded directly from TensorFlow and has already been divided.\n",
    "\n",
    "* x_train is the dataset of 28x28 images of handwritten digits that the model will be trained on\n",
    "* y_train is the dataset of labels that correspond to x_train.\n",
    "* x_test is the dataset of 28x28 images of handwritten digits that the model will be tested on\n",
    "* y_test is the dataset of labels that correspond to x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe26d45-f183-448d-b270-bb5c849236c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b477e-d0d5-4e56-9bd0-9ce49b685905",
   "metadata": {},
   "source": [
    "Let see the counts of each digit present in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6516c-868b-4cdf-824e-05bcd74a6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a1dd4-1404-4713-a324-10ec5dd61697",
   "metadata": {},
   "source": [
    "There are similar counts for each digit.\n",
    "This is good as the ***model*** will have enough images for each class to train the features for each class.\n",
    "There is no need to downsample or upweigh.\n",
    "\n",
    "PTFIXME: 1 pic for every number? WTF ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c9eb6-57c6-4ad1-9c58-fafee90c9e28",
   "metadata": {},
   "source": [
    "## 2.2 Check for NaN Values\n",
    "There shell be no NaN values in the dataset.\n",
    "There is no need to preprocess the data to deal with Nan's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca12506-1791-4e32-b7a4-210508e1e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False == np.isnan(x_train).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704a2a1-0de0-4f04-9830-7fcfcca8529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False == np.isnan(x_test).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8958a6-72de-4173-911a-76b81cc34dd7",
   "metadata": {},
   "source": [
    "## 2.3 Normalization and Reshaping\n",
    "Since the values in the x_train dataset are 28x28 images, the input shape must be specified so that the model will know what is being inputed.\\\n",
    "The first convolution layer expects a single 60000x28x28x1 tensor instead of 60000 28x28x1 tensors.\n",
    "\n",
    "Models generally run better on normalized values.\\\n",
    "The best way to normalize the data depends on each individual dataset.\\\n",
    "For the MNIST dataset, I want each value to be between 0.0 and 1.0.\\\n",
    "As all values originally fall under the 0.0-255.0 range, divide by 255.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f578c-0886-460f-9c4d-f480a8604c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "\n",
    "x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_train=x_train / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "x_test=x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84d351-e5a9-4efd-b530-e7ae10a91085",
   "metadata": {},
   "source": [
    "## 2.4 Label Encoding\n",
    "The labels for the training and the testing dataset are currently categorical and is not continuous.\n",
    "To include categorical dataset in the ***model***, the labels should be converted to one-hot encodings.\n",
    "\n",
    "For example, 2 becomes [0,0,1,0,0,0,0,0,0,0] and 7 becomes [0,0,0,0,0,0,0,1,0,0].\n",
    "\n",
    "Transforming the labels into one-hot encodings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39e778-9018-4eb2-84dd-459556bebacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n",
    "y_test = tf.one_hot(y_test.astype(np.int32), depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dba001-1efb-49dc-b7f8-d4790ceb6b8f",
   "metadata": {},
   "source": [
    "## 2.5 Visualize Data\n",
    "Visualizing an image in the dataset.\n",
    "The image is an image of a handwritten 5.\n",
    "The one-hot encoding holds the value of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96549db8-9c4d-426e-8132-3ec123348a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[100][:,:,0])\n",
    "print(y_train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c830003e-fe47-4720-8f0d-a9f358e22868",
   "metadata": {},
   "source": [
    "# 3. CNN\n",
    "Time to build the CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04fb14-d515-4391-b2a3-e93279f9c98b",
   "metadata": {},
   "source": [
    "## 3.1 Define the model\n",
    "Have to define now\n",
    "* batch_size\n",
    "* num_classes,\n",
    "* and epochs.\n",
    "\n",
    "Try changing the values and test how different values affect the accuracy of the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c83dc2c-f62a-48be-bc25-94ec89d461a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef735698-efe9-4be9-a2f5-94921c8a3eee",
   "metadata": {},
   "source": [
    "Building the ***model***.\\\n",
    "The ***model*** contains various layers stacked on top of each other.\\\n",
    "The output of one layer feeds into the input of the next layer.\n",
    "\n",
    "Conv2D layers are convolutions.\\\n",
    "Each filter (32 in the first two convolution layers and 64 in the next two convolution layers) transforms a part of the image (5x5 for the first two Conv2D layers and 3x3 for the next two Conv2D layers).\\\n",
    "The transformation is applied on the whole image.\n",
    "\n",
    "***MaxPool2D*** is a downsampling filter.\\\n",
    "It reduces a 2x2 matrix of the image to a single pixel with the maximum value of the 2x2 matrix.\\\n",
    "The filter aims to conserve the main features of the image while reducing the size.\n",
    "\n",
    "***Dropout*** is a regularization layer.\\\n",
    "In this ***model***, 25% of the nodes in the layer are randomly ignores, allowing the network to learn different features.\\\n",
    "This prevents overfitting.\n",
    "\n",
    "***relu*** is the rectifier, and it is used to find nonlinearity in the data.\\\n",
    "It works by returning the input value if the input value >= 0.\\\n",
    "If the input is negative, it returns 0.\n",
    "\n",
    "***Flatten*** converts the tensors into a 1D vector.\n",
    "\n",
    "The ***Dense*** layers are an artificial neural network (ANN).\\\n",
    "The last layer returns the probability that an image is in each class (one for each digit).\n",
    "\n",
    "As this ***model*** aims to categorize the images, I will use a categorical_crossentropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43915722-44f9-4a24-92b7-50f24a385c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(strides=(2,2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d522e-885b-42e1-88d9-f5548b2e3f4d",
   "metadata": {},
   "source": [
    "## 3.2 Fit the Training Data\n",
    "The next step is to fit the training data.\\\n",
    "If I achieve a certain level of accuracy, it may not be necessary to continue training the model, especially if time and resources are limited.\n",
    "\n",
    "Lets define a ***DoneCallBack*** so that if 99.5% accuracy is achieved, the ***model*** stops training.\\\n",
    "The ***model*** is not likely to stop prematurely if only 5 epochs are specified.\\\n",
    "Try it out with more epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7194d-ee14-40ed-95d7-9536f9c41180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoneCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('acc')>0.995):\n",
    "      print(\"\\nReached 99.5% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = DoneCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1970611-15b5-44aa-b3c7-70bee549fa80",
   "metadata": {},
   "source": [
    "Testing the ***model*** on a validation dataset prevents overfitting of the data.\\\n",
    "Lets' specify a 10% validation and 90% training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179e18e-7883-4de9-a233-d771a62d9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af300961-2a7c-4124-b4bd-e11aba7a4c2e",
   "metadata": {},
   "source": [
    "# 4. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd77b7-5fc1-498b-9396-0cc7416a0dd4",
   "metadata": {},
   "source": [
    "## 4.1 Loss and Accuracy Curves\n",
    "Let's evaluate the loss and accuracy of the ***model***.\\\n",
    "The accuracy increases over time and the loss decreases over time.\\\n",
    "However, the accuracy of the validation set seems to slightly decrease towards the end even thought our training accuracy increased.\\\n",
    "Running the ***model*** for more epochs might cause the ***model*** to be susceptible to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c24c86-4a64-401a-b2ba-79167f1c33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\n",
    "#ax[0].plot(history.history['val_loss'], color='r', label=\"Validation Loss\",axes =ax[0])\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"Validation Loss\")\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['acc'], color='b', label=\"Training Accuracy\")\n",
    "ax[1].plot(history.history['val_acc'], color='r',label=\"Validation Accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474ff12-e4a4-41dd-9be9-319d04edf19a",
   "metadata": {},
   "source": [
    "The accuracy increases over time and the loss decreases over time.\\\n",
    "However, the accuracy of the validation set seems to slightly decrease towards the end even thought the Training Accuracy increased.\\\n",
    "Running the ***model*** for more epochs might cause the ***model*** to be susceptible to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98554fe-922c-4c44-b010-640524f21ed6",
   "metadata": {},
   "source": [
    "## 4.2 Predict Results\n",
    "The ***model*** runs pretty well, with an accuracy around 99.3% on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f842b-8206-4d07-83f8-591c585d6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7053f0-adee-4786-ba5b-17a7ebfc6625",
   "metadata": {},
   "source": [
    "## 4.3 Confusion Matrix\n",
    "Let's compute and plot a Confusion Matrix using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b61af-01c0-4250-950c-1f8f84331d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the testing dataset\n",
    "Y_pred = model.predict(x_test)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert testing observations to one hot vectors\n",
    "Y_true = np.argmax(y_test,axis = 1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(Y_true, Y_pred_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d26c48-4988-4301-8802-7f74b10e6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775a499-4dbb-4630-952c-40b89986aa97",
   "metadata": {},
   "source": [
    "Crossing X and Y axises here contains a number, the higher the number(error) - the higher confusion between these 2 numbers, i. e. probabily of error.\n",
    "\n",
    "Let's agree, by saying ***{1,9}*** I mean a cross of X equal to 1 and Y equal to 9 axises.\n",
    "\n",
    "Here seems to be a slightly higher confusion between ***{2,7}*** which is ***7*** and ***{3,5}*** which is ***7*** again.\n",
    "\n",
    "This is reasonable:\n",
    "* 2's and 7's might look similar if 2's tail is missing\n",
    "* 3's and 5's can be mistaken when the top side is unclear written\n",
    "\n",
    "As opposite ***{1,9}*** is ***0*** - yeah, 1 and 9 are hardly confusable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
